# Use an official Python base image
FROM python:3.10-slim

# Set working directory
WORKDIR /app

# Set environment variables (can be overridden)
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    # Set default MLflow tracking URI if not provided externally
    MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI} \
    # Default model names/stage (can be overridden by K8s env vars)
    MODEL_NAME_CF=${MODEL_NAME_CF:-spark-als-recommender} \
    MODEL_NAME_CB=${MODEL_NAME_CB:-sklearn-tfidf-vectorizer} \
    MODEL_STAGE=${MODEL_STAGE:-Production} \
    # Cloud credentials should be handled via instance roles or K8s secrets, not baked in
    # GOOGLE_APPLICATION_CREDENTIALS=${GOOGLE_APPLICATION_CREDENTIALS} \
    DATA_BUCKET=${DATA_BUCKET} \
    MODEL_ARTIFACT_BUCKET=${MODEL_ARTIFACT_BUCKET}

# Install system dependencies if needed (e.g., for certain ML libraries)
# RUN apt-get update && apt-get install -y --no-install-recommends some-package && rm -rf /var/lib/apt/lists/*

# Copy requirements first to leverage Docker cache
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application code
COPY ./src /app/src
COPY ./configs /app/configs

# Expose the port the app runs on
EXPOSE 8000

# Command to run the application using Uvicorn
# Use multiple workers in production (e.g., based on CPU cores)
# CMD ["uvicorn", "src.serving.api:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
CMD ["uvicorn", "src.serving.api:app", "--host", "0.0.0.0", "--port", "8000"]